{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c4c8b7",
   "metadata": {},
   "source": [
    "# Decomposer Test\n",
    "\n",
    "This notebook is made for assessing the performence of the decomposer alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3a957",
   "metadata": {},
   "source": [
    "## Generate QD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4217b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from processors import ProcessorRegistry\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generation_utils import to_model_prompt, generate_batch\n",
    "from inference.qpl.text_to_qpl import get_decomposer_generation_params, DecomposerMode\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 8\n",
    "MAX_NEW_TOKENS = 256\n",
    "MODEL_PATH = \"output/gemma-3-4b-it-question_decomposer_ds_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-20958\"\n",
    "DATASET_ID = \"bgunlp/question_decomposer_ds\"\n",
    "MAX_RETRIES = 3\n",
    "MODE = DecomposerMode.GREEDY\n",
    "\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(MODEL_PATH).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = list(load_dataset(DATASET_ID, split=\"validation\"))\n",
    "processor = ProcessorRegistry.get(DATASET_ID)(train=False)\n",
    "chat_templates = list(map(processor.to_chat_template, test_dataset))\n",
    "prompts = list(map(lambda ct: to_model_prompt(tokenizer, ct), chat_templates))\n",
    "\n",
    "# Decompose questions\n",
    "outputs = generate_batch(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_prompts=prompts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    progress_bar=tqdm(total=len(prompts), desc=\"Decomposing\"),\n",
    "    is_valid_output=(lambda i, output: test_dataset[i]['question'] != output) if MODE in [DecomposerMode.FIRST_GREEDY, DecomposerMode.SAMPLING] else None,\n",
    "    max_retries=MAX_RETRIES,\n",
    "    **get_decomposer_generation_params(MODE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddf002",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "op_correct = 0\n",
    "sum_similarity = 0\n",
    "sentences_count = 0\n",
    "\n",
    "op_to_id = {\n",
    "    'aggregate': 0,\n",
    "    'except': 1,\n",
    "    'filter': 2,\n",
    "    'intersect': 3,\n",
    "    'join': 4,\n",
    "    'scan': 5,\n",
    "    'sort': 6,\n",
    "    'topsort': 7,\n",
    "    'union': 8,\n",
    "    'other': 9\n",
    "}\n",
    "id_to_op = {v: k for k, v in op_to_id.items()}\n",
    "\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "processor = ProcessorRegistry.get(DATASET_ID)(train=True)\n",
    "chat_templates = []\n",
    "for example in test_dataset:\n",
    "    chat_templates.append(processor.to_chat_template(example))\n",
    "\n",
    "for out, chat_template in tqdm(zip(outputs, chat_templates), desc=\"Evaluating\", total=len(outputs)):\n",
    "    if chat_template is None:\n",
    "        continue  # Skip examples that caused recursion errors\n",
    "    \n",
    "    # process labels\n",
    "    label = chat_template['messages'][-1]['content']\n",
    "    model_lines = out.split(\"\\n\")\n",
    "    label_lines = label.split(\"\\n\")\n",
    "\n",
    "    # operator classification\n",
    "    model_op_id = op_to_id.get(model_lines[0].lower(), op_to_id[\"other\"])\n",
    "    label_op_id = op_to_id.get(label_lines[0].lower(), op_to_id[\"other\"])\n",
    "    y_pred.append(model_op_id)\n",
    "    y_true.append(label_op_id)\n",
    "\n",
    "    # sentence similarity\n",
    "    if model_op_id == label_op_id:\n",
    "        op_correct += 1\n",
    "\n",
    "        model_sentences = model_lines[1:]\n",
    "        label_sentences = label_lines[1:]\n",
    "\n",
    "        sentences_count += len(label_sentences)\n",
    "\n",
    "        if len(model_sentences) != len(label_sentences):\n",
    "            print(\"======================\")\n",
    "            print(out)\n",
    "            print(\"----\")\n",
    "            print(label)\n",
    "            print(\"======================\")\n",
    "        else:\n",
    "            all_sentences = model_sentences + label_sentences\n",
    "            embeddings = emb_model.encode(all_sentences, show_progress_bar=False)\n",
    "            similarity_matrix = embeddings @ embeddings.T\n",
    "            if len(model_sentences) == 0:\n",
    "                similarity = 0\n",
    "            elif len(model_sentences) == 1:\n",
    "                similarity = similarity_matrix[0][1]\n",
    "            else:\n",
    "                similarity = max(\n",
    "                    similarity_matrix[0,2] + similarity_matrix[1,3],\n",
    "                    similarity_matrix[0,3] + similarity_matrix[1,2]\n",
    "                ) / 2\n",
    "            sum_similarity += similarity\n",
    "\n",
    "print(f\"Operator Accuracy: {op_correct / len(test_dataset)}\")\n",
    "print(f\"Sentence Similarity (when operator is correct): {sum_similarity / op_correct}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Print nicely\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.index = df.index.map(lambda x: id_to_op[int(x)] if x.isdigit() else x)\n",
    "df['support'] = df['support'].astype(int)\n",
    "print(df)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(op_to_id.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(op_to_id.keys()))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
