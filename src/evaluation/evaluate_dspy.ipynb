{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949b3cf4",
   "metadata": {},
   "source": [
    "# Evaluate Full Pipeline Using The DsPy Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a074d61",
   "metadata": {},
   "source": [
    "## Preperations\n",
    "\n",
    "Before starting, activate the mlflow server by running `mlflow server --backend-store-uri sqlite:///mydb.sqlite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import mlflow\n",
    "import pyodbc\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.experiments.qpl.text_to_qpl_dspy import TextToQPL, SchemaRepresentation, connection_string\n",
    "\n",
    "# start mlflow\n",
    "mlflow.dspy.autolog(\n",
    "    log_compiles=True,\n",
    "    log_evals=True,\n",
    "    log_traces_from_compile=False,\n",
    "    log_traces_from_eval=True,\n",
    ")\n",
    "\n",
    "# Configure MLflow tracking\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # Use local MLflow server\n",
    "mlflow.set_experiment(\"QPL\")\n",
    "\n",
    "# connect to db\n",
    "conn = pyodbc.connect(connection_string, autocommit=True)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# load program\n",
    "SCHEMA_REPR = SchemaRepresentation.M_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862e689",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipeline test\n",
    "eval_dataset = [\n",
    "    dspy.Example(\n",
    "        db_id=row['qpl'].split('|')[0].strip(),\n",
    "        question=row['question'],\n",
    "        query=row['query']\n",
    "    ).with_inputs('db_id', 'question')\n",
    "    for row in load_dataset(\"d4nieldev/nl2qpl-ds\", split=\"development\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c63220",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load program\n",
    "text_to_qpl = TextToQPL(schema_repr=SCHEMA_REPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric\n",
    "from src.experiments.qpl.validate_qpl import compare_qpl_sql\n",
    "\n",
    "# define metric (execution accuracy)\n",
    "def metric(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    gold_sql = example['query']\n",
    "    db_id = example['db_id']\n",
    "    predicted_qpl = prediction['qpl']\n",
    "    correct, err = compare_qpl_sql(qpl=predicted_qpl, sql=gold_sql, db_id=db_id, cursor=cursor)\n",
    "    print(correct, err)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d65049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate program\n",
    "\n",
    "# configure model\n",
    "dspy.settings.configure(lm=dspy.LM(\"openai/gpt-5-mini\", temperature=1.0, max_tokens=20000))\n",
    "\n",
    "# define evaluation recipe\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=eval_dataset[:1],\n",
    "    metric=metric,\n",
    "    num_threads=4,\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    "    provide_traceback=True\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "results = evaluate(text_to_qpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815ab22",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ef5ec",
   "metadata": {},
   "source": [
    "### Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45387e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposer train\n",
    "from src.utilsema import DBSchema\n",
    "\n",
    "schemas = DBSchema.from_db_schemas_file()\n",
    "\n",
    "decomposer_ds = load_dataset(\"d4nieldev/qpl-decomposer-cot-ds\", 'original', split=\"train\")\n",
    "q_to_row = {\n",
    "    row['question']: row\n",
    "    for row in decomposer_ds\n",
    "}\n",
    "\n",
    "def is_err(func, *args, **kwargs):\n",
    "    try:\n",
    "        func(*args, **kwargs)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return True\n",
    "\n",
    "def construct_qd(row) -> dict:\n",
    "    sub_questions = [sq for sq in [row['sub_question_1'], row['sub_question_2']] if sq]\n",
    "    return {\n",
    "        \"question\": row['question'],\n",
    "        \"operator\": row['op'],\n",
    "        \"arguments\": [\n",
    "            construct_qd(q_to_row[sq]) for sq in sub_questions\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "decomposer_trainset = [\n",
    "    dspy.Example(\n",
    "        db_schema=schemas[row['db_id']].m_schema() if SCHEMA_REPR == SchemaRepresentation.M_SCHEMA else schemas[row['db_id']].ddl(),\n",
    "        question=row['question'],\n",
    "\n",
    "        qd=construct_qd(row),\n",
    "\n",
    "        reasoning=row['cot'],\n",
    "        operator=row['op'],\n",
    "        sub_questions=[sq for sq in [row['sub_question_1'], row['sub_question_2']] if sq]\n",
    "    ).with_inputs('db_schema', 'question')\n",
    "    for row in decomposer_ds\n",
    "    if not is_err(construct_qd, row)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completer trainset\n",
    "import re\n",
    "\n",
    "flat_qpl_scan_pattern = re.compile(\n",
    "    r\"#(?P<idx>\\d+) = Scan Table \\[ (?P<table>\\w+) \\]( Predicate \\[ (?P<pred>[^\\]]+) \\])?( Distinct \\[ (?P<distinct>true) \\])? Output \\[ (?P<out>[^\\]]+) \\]\"\n",
    ")\n",
    "flat_qpl_line_pattern = re.compile(\n",
    "    r\"#(?P<idx>\\d+) = (?P<op>\\w+) \\[ (?P<ins>[^\\]]+) \\] ((?P<opt>\\w+) \\[ (?P<arg>[^\\]]+) \\] )*Output \\[ (?P<out>[^\\]]+) \\]\"\n",
    ")\n",
    "\n",
    "def line_prefix(qpl_line: str) -> str:\n",
    "    if (m := flat_qpl_scan_pattern.match(qpl_line)):\n",
    "        return f\"#{m.group('idx')} = Scan Table\"\n",
    "    elif (m := flat_qpl_line_pattern.match(qpl_line)):\n",
    "        return f\"#{m.group('idx')} = {m.group('op')} [ {m.group('ins')} ]\"\n",
    "    raise ValueError(f\"Could not parse QPL line: {qpl_line}\")\n",
    "\n",
    "\n",
    "completer_trainset = [\n",
    "    dspy.Example(\n",
    "        db_schema=schemas[row['db_id']].m_schema() if SCHEMA_REPR == SchemaRepresentation.M_SCHEMA else schemas[row['db_id']].ddl(),\n",
    "        question=row['question'],\n",
    "        prefix_qpl=row['prefix_qpl']+\"\\n\"+line_prefix(row['qpl_line'])+\" ...\",\n",
    "\n",
    "        operator=row['op'],\n",
    "        db_id=row['db_id'],\n",
    "\n",
    "        reasoning=row['cot'],\n",
    "        last_line=row['qpl_line']\n",
    "    ).with_inputs('db_schema', 'question', 'prefix_qpl')\n",
    "    for row in load_dataset(\"d4nieldev/qpl-completer-cot-ds\", 'original', split=\"train\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subsets for more efficient training\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(42)  # for reproduction\n",
    "\n",
    "def sample_trainset(trainset: list[dspy.Example], n_per_op: int = 10, n_splits: int = 1) -> tuple[list[dspy.Example]]:\n",
    "    groups = defaultdict(list)\n",
    "    for example in trainset:\n",
    "        if example[\"operator\"] != \"Top\":\n",
    "            groups[example[\"operator\"]].append(example)\n",
    "    \n",
    "    # shuffle examples in every group\n",
    "    for op in groups:\n",
    "        assert len(groups[op]) > n_per_op * n_splits, f\"Not enough examples for operator {op}: {len(groups[op])} < {n_per_op * n_splits}\"\n",
    "        random.shuffle(groups[op])\n",
    "\n",
    "    # split accordingly\n",
    "    return tuple([[example for examples in groups.values() for example in examples[i*n_per_op:(i+1)*n_per_op]] for i in range(n_splits)])\n",
    "\n",
    "decomposer_trainset, decomposer_valset = sample_trainset(decomposer_trainset, n_per_op=5, n_splits=2)\n",
    "completer_trainset, completer_valset = sample_trainset(completer_trainset, n_per_op=5, n_splits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de356464",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define completer metric\n",
    "from src.experiments.qpl.validate_qpl import flat_qpl_to_cte, execute_sql, same_rs\n",
    "\n",
    "def qpl_to_flat(qpl: str) -> list[str]:\n",
    "    lines = [line[:line.index(';')] if ';' in line else line for line in qpl.split('\\n')]\n",
    "    return [l for l in lines if l.strip()]\n",
    "\n",
    "def completer_metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    db_id = gold['db_id']\n",
    "    actual_prefix = \"\\n\".join(gold['prefix_qpl'].split(\"\\n\")[:-1])\n",
    "    gold_qpl = actual_prefix+\"\\n\"+gold['last_line']\n",
    "    pred_qpl = actual_prefix+\"\\n\"+pred['last_line']\n",
    "\n",
    "    same = False\n",
    "    feedback_lines = []\n",
    "    err = False\n",
    "    \n",
    "    try:\n",
    "        # convert QPL to CTE\n",
    "        pred_cte = flat_qpl_to_cte(qpl_to_flat(pred_qpl), db_id)\n",
    "    except Exception as e:\n",
    "        feedback_lines.append(\"Error converting QPL to CTE - \" + str(e))\n",
    "        feedback_lines.append(\"\")\n",
    "        err = True\n",
    "    else:\n",
    "        # execute and compare\n",
    "        feedback_lines.append(f\"Converted QPL to CTE: `{pred_cte}`.\")\n",
    "        feedback_lines.append(\"\")\n",
    "        try:\n",
    "            prs = execute_sql(cursor, pred_cte)\n",
    "        except Exception as e:\n",
    "            feedback_lines.append(\"Could not execute predicted SQL: \" + str(e))\n",
    "            feedback_lines.append(\"\")\n",
    "            err = True\n",
    "        else:\n",
    "            grs = execute_sql(cursor, flat_qpl_to_cte(qpl_to_flat(gold_qpl), db_id))\n",
    "            same = same_rs(grs, prs, qpl_to_flat(pred_qpl))\n",
    "\n",
    "    feedback_lines.append(f\"Your answer is **{'correct' if same else ('incorrect' if not err else 'invalid')}**. The best QPL line is: `{gold['last_line']}`\")\n",
    "    feedback_lines.append(\"\")\n",
    "    feedback_lines.append(f\"Here is the full step-by-step reasoning that led to that conclusion:\")\n",
    "    feedback_lines.append(gold['reasoning'])\n",
    "    feedback_lines.append(\"Think about what takeaways you can learn from this solution to improve your future answers and approach to similar problems.\")\n",
    "    \n",
    "    # NOTE: maybe add the execution results? (can be very big)\n",
    "    # NOTE: maybe give higher scores to queries that are executable but incorrect\n",
    "\n",
    "    return dspy.Prediction(score=1 if same else 0, feedback=\"\\n\".join(feedback_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad95f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposer judge\n",
    "import dspy\n",
    "from src.utilsema import DBSchema\n",
    "\n",
    "dspy.settings.configure(lm=dspy.LM(\"openai/gpt-5-mini\", temperature=1.0, max_tokens=20000))\n",
    "\n",
    "schemas = DBSchema.from_db_schemas_file()\n",
    "\n",
    "class DecomposerJudge(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of an immediate question decomposition into a toplevel QPL operator and its arguments in the form of natural language questions.\n",
    "    \n",
    "The toplevel QPL operators are:\n",
    "- **Scan** - Scan all rows in a table with optional filtering predicate (no decomposition needed - the question is atomic)\n",
    "- **Aggregate** - Aggregate a stream of tuples, optionally using a grouping criterion into a stream of groups (1 sub-question)\n",
    "- **Filter** - Remove tuples from a stream that do not match a predicate (1 sub-question)\n",
    "- **Sort** - Sort a stream according to a sorting expression (1 sub-question)\n",
    "- **TopSort** - Select the top-K tuples from a stream according to a sorting expression (1 sub-question)\n",
    "- **Join** - Perform a logical join operation between two streams based on a join condition (2 sub-questions)\n",
    "- **Except** - Compute the set difference between two streams of tuples (2 sub-questions)\n",
    "- **Intersect** - Compute the set intersection between two streams of tuples (2 sub-questions)\n",
    "- **Union** - Compute the set union between two streams of tuples (2 sub-questions)\n",
    "\n",
    "Notes:\n",
    "- The operation determined by the operator and arguments predicted in the immediate sub questions are what will determine how to answer the question. Make sure that no critical details are omitted in the arguments (sub questions) unless covered by the operator.\n",
    "- Even if the immediate decomposition does not match the first step in the full decomposition, you should take a look at what happens down the tree because some operations can be done in different orders and still be valid.\n",
    "- The sub-questions should not contain the entire decomposition plan, and should be simple questions, that will later be decomposed in the same way.\n",
    "\"\"\"\n",
    "\n",
    "    db_schema: str = dspy.InputField(desc=\"The schema of the database the question is being asked about\")\n",
    "    question: str = dspy.InputField(desc=\"The question to be decomposed\")\n",
    "    full_question_decomposition: dict = dspy.InputField(desc=\"The full question decomposition tree, where the first element is the gold immediate decomposition, but it also includes the decomposition for the arguments (recursively), which is not required but can provide context.\")\n",
    "    immediate_question_decomposition: dict = dspy.InputField(desc=\"The decomposition only for the immediate sub-questions, including only the toplevel operator and its arguments. This is what is being evaluated.\")\n",
    "\n",
    "    score: float = dspy.OutputField(desc=\"A score between 0 and 1 indicating how good the immediate question decomposition is.\")\n",
    "\n",
    "\n",
    "judge = dspy.ChainOfThought(DecomposerJudge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ba2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad example\n",
    "result = judge(\n",
    "    db_schema=schemas['concert_singer'],\n",
    "    question=\"Find the number of concerts happened in the stadium with the highest capacity .\",\n",
    "    full_question_decomposition={\n",
    "        \"operator\": \"Aggregate\",\n",
    "        \"arguments\": [\n",
    "            {\n",
    "                \"question\": \"List 1 for each concert that happened in the stadium with the highest capacity.\",\n",
    "                \"operator\": \"Join\",\n",
    "                \"arguments\": [\n",
    "                    {\n",
    "                        \"question\": \"Find the id of the stadium with the highest capacity.\",\n",
    "                        \"operator\": \"TopSort\",\n",
    "                        \"arguments\": [\n",
    "                            {\n",
    "                                \"question\": \"Find the id and capacity of all stadiums.\",\n",
    "                                \"operator\": \"Scan\",\n",
    "                                \"arguments\": []\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"question\": \"Find the id of the stadium of all concerts.\",\n",
    "                        \"operator\": \"Scan\",\n",
    "                        \"arguments\": []\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    immediate_question_decomposition={\n",
    "        \"operator\": \"Join\",\n",
    "        \"arguments\": [\n",
    "            {\"question\": \"Find the id of the stadium with the highest capacity.\"},\n",
    "            {\"question\": \"Find the stadium ids of all concerts.\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.reasoning)\n",
    "print(result.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good example\n",
    "result = judge(\n",
    "    db_schema=schemas['concert_singer'],\n",
    "    question=\"Find the number of concerts happened in the stadium with the highest capacity .\",\n",
    "    full_question_decomposition={\n",
    "        \"operator\": \"Aggregate\",\n",
    "        \"arguments\": [\n",
    "            {\n",
    "                \"question\": \"List 1 for each concert that happened in the stadium with the highest capacity.\",\n",
    "                \"operator\": \"Join\",\n",
    "                \"arguments\": [\n",
    "                    {\n",
    "                        \"question\": \"Find the id of the stadium with the highest capacity.\",\n",
    "                        \"operator\": \"TopSort\",\n",
    "                        \"arguments\": [\n",
    "                            {\n",
    "                                \"question\": \"Find the id and capacity of all stadiums.\",\n",
    "                                \"operator\": \"Scan\",\n",
    "                                \"arguments\": []\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"question\": \"Find the id of the stadium of all concerts.\",\n",
    "                        \"operator\": \"Scan\",\n",
    "                        \"arguments\": []\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    immediate_question_decomposition={\n",
    "        \"operator\": \"Join\",\n",
    "        \"arguments\": [\n",
    "            {\"question\": \"Find the id of the stadium with the highest capacity.\"},\n",
    "            {\"question\": \"For each stadium id, find how many concerts happened in it.\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.reasoning)\n",
    "print(result.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a655e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposer metric\n",
    "import json\n",
    "\n",
    "def decomposer_metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    immediate_decomposition = {\n",
    "        \"question\": gold.question,\n",
    "        \"operator\": pred.operator,\n",
    "        \"arguments\": [{\"question\": sq} for sq in pred.sub_questions]\n",
    "    }\n",
    "    judge_result = judge(\n",
    "        db_schema=gold.db_schema,\n",
    "        question=gold.question,\n",
    "        full_question_decomposition=gold.qd,\n",
    "        immediate_question_decomposition=immediate_decomposition\n",
    "    )\n",
    "\n",
    "    feedback = \"Full correct decomposition tree:\\n\"+json.dumps(gold.qd, indent=2)\n",
    "    feedback += \"\\n\\nYour immediate decomposition:\\n\"+json.dumps(immediate_decomposition, indent=2)\n",
    "    feedback += \"\\n\\nEvaluation of your decomposition:\\n\"+judge_result.reasoning\n",
    "\n",
    "    return dspy.Prediction(score=judge_result.score, feedback=feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b4de7",
   "metadata": {},
   "source": [
    "### GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize decomposer\n",
    "decomposer_optimizer = dspy.GEPA(\n",
    "    metric=decomposer_metric_with_feedback,\n",
    "    auto=\"light\",\n",
    "    num_threads=4,\n",
    "    track_stats=True,\n",
    "    reflection_lm=dspy.LM(\"openai/gpt-5\", temperature=1.0, max_tokens=20000)\n",
    ")\n",
    "\n",
    "text_to_qpl.decomposer.compile(optimizer=decomposer_optimizer, trainset=decomposer_trainset, valset=decomposer_valset)\n",
    "text_to_qpl.decomposer.save(\"output/dspy/decomposer_gepa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ed45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize completer\n",
    "completer_optimizer = dspy.GEPA(\n",
    "    metric=completer_metric_with_feedback,\n",
    "    auto=\"light\",\n",
    "    num_threads=4,\n",
    "    track_stats=True,\n",
    "    reflection_lm=dspy.LM(\"openai/gpt-5\", temperature=1.0, max_tokens=20000)\n",
    ")\n",
    "\n",
    "text_to_qpl.completer.compile(optimizer=completer_optimizer, trainset=completer_trainset, valset=completer_valset)\n",
    "text_to_qpl.completer.save(\"output/dspy/completer_gepa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate optimized program\n",
    "\n",
    "text_to_qpl.decomposer.load(\"output/dspy/decomposer_gepa.json\")\n",
    "text_to_qpl.completer.load(\"output/dspy/completer_gepa.json\")\n",
    "\n",
    "# define evaluation recipe\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=eval_dataset,\n",
    "    metric=metric,\n",
    "    num_threads=32,\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    "    provide_traceback=True\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "results = evaluate(text_to_qpl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
