{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c4c8b7",
   "metadata": {},
   "source": [
    "# Decomposer Test\n",
    "\n",
    "This notebook is made for assessing the performence of the decomposer alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3a957",
   "metadata": {},
   "source": [
    "## Generate QD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4217b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.prompters import QPLDecomposerCotPrompter\n",
    "from src.utils.generation import to_model_prompt, generate_batch\n",
    "import src.utils.paths as p\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 8\n",
    "MAX_NEW_TOKENS = 8192\n",
    "# MODEL_DIR = \"qwen-3-qpl-cot\"\n",
    "# MODEL_CKPT = MODEL_DIR + \"/checkpoint-2864\"\n",
    "# MODEL_PATH = p.TRAINED_MODELS_DIR / MODEL_CKPT\n",
    "# DATASET_ID = \"bgunlp/question_decomposer_ds\"\n",
    "MODEL_PATH = \"Qwen/Qwen3-4B\"\n",
    "DATASET_ID = \"d4nieldev/qpl-decomposer-cot-ds\"\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Load and process data\n",
    "prompter = QPLDecomposerCotPrompter(with_assistant=True, schema_representation=\"m_schema\")\n",
    "test_dataset = list(prompter.load_dataset()['validation'])\n",
    "chat_templates = list(map(prompter.to_chat_template, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1344d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc796fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = chat_templates[13]\n",
    "print(chat_template['messages'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template['messages'][1]['content'] = \"\"\"【DB_ID】concert_singer\n",
    "【Schema】\n",
    "# Table: singer (6 rows)\n",
    "## Columns (name:type | pk? | description? | metadata? | fk? | examples?)\n",
    "[\n",
    "(Name: TEXT | This column has 0 null values and 6 distinct values. The values are between 9 and 12 characters long, with a mean of 10.50 and a standard deviation of 1.38. | Values Examples: [Timbaland, John Nizinik, Justin Brown, Joe Sharp, Rose White]),\n",
    "(Song_release_year: TEXT | This column has 0 null values and 6 distinct values. All of them are numeric. The values are always 4 characters long. Common prefixes: 201 (3 occurrences), 200 (2 occurrences). | Values Examples: [2013, 2014, 2003, 2008, 1992]),\n",
    "(Age: NUMBER | This column has 0 null values and 6 distinct values. All of them are numeric. The values are between 25.0 and 52.0, with a mean of 37.00 and a standard deviation of 10.10. | Values Examples: [29, 41, 32, 43, 52]),\n",
    "]\n",
    "\n",
    "【Foreign Keys】\n",
    "concert.Stadium_ID=stadium.Stadium_ID\n",
    "singer_in_concert.Singer_ID=singer.Singer_ID\n",
    "singer_in_concert.concert_ID=concert.concert_ID\n",
    "\n",
    "\n",
    "[Question]: Show the name and the release year of the song by the youngest singer.\n",
    "\n",
    "First, determine the operator, then formulate the sub-questions (unless the operator is \"Scan\", in which case no sub-questions are needed and this step must be skipped), and finally justify the decomposition.\n",
    "Provide your reasoning enclosed in <think> and </think> tags, and afterwards provide the final answer in the following format: the first line of the final answer should be the toplevel operator, the following lines should be the predicted sub-questions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd279ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.apply_chat_template(\n",
    "    [msg for msg in chat_template[\"messages\"] if msg['role'] in ['system', 'user']],  # only the system and user\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    continue_final_message=False,\n",
    ")\n",
    "ids = tokenizer(\n",
    "    ids,\n",
    "    padding=True,\n",
    "    padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**ids, max_new_tokens=4096, labels=ids.input_ids, temperature=0.6, top_p=0.95, top_k=20)\n",
    "# preplexity = torch.exp(outputs.loss)\n",
    "# print(f\"Perplexity: {preplexity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c7ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model & tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "prompts = list(map(lambda ct: to_model_prompt(tokenizer, ct), chat_templates))\n",
    "\n",
    "# Decompose questions\n",
    "predictions = generate_batch(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_prompts=prompts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    progress_bar=tqdm(total=len(prompts), desc=\"Decomposing\"),\n",
    "    max_retries=MAX_RETRIES,\n",
    "    **get_decomposer_generation_params(MODE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddf002",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "op_correct = 0\n",
    "sum_similarity = 0\n",
    "sentences_count = 0\n",
    "\n",
    "op_to_id = {\n",
    "    'aggregate': 0,\n",
    "    'except': 1,\n",
    "    'filter': 2,\n",
    "    'intersect': 3,\n",
    "    'join': 4,\n",
    "    'scan': 5,\n",
    "    'sort': 6,\n",
    "    'topsort': 7,\n",
    "    'union': 8,\n",
    "    'other': 9\n",
    "}\n",
    "id_to_op = {v: k for k, v in op_to_id.items()}\n",
    "\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "prompter = QPLDecomposerCotPrompter(with_assistant=True)\n",
    "chat_templates = [prompter.to_chat_template(example) for example in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from predictions\n",
    "output_pattern = re.compile(r\"(?P<reasoning><think>.*?</think>)?\\s*(?P<answer>.*)\", re.DOTALL)\n",
    "\n",
    "output_json = []\n",
    "for pred, chat_template in tqdm(zip(predictions, chat_templates), desc=\"Evaluating\", total=len(predictions)):\n",
    "    if pred is None:\n",
    "        print(\"No prediction for chat template:\", chat_template)\n",
    "        continue\n",
    "\n",
    "    # save prediction vs. gold\n",
    "    output_json.append({\n",
    "        \"input\": [ct for ct in chat_template['messages'] if ct['role'] in ['system', 'user']],\n",
    "        \"pred\": pred,\n",
    "        \"gold\": chat_template['messages'][-1]['content']\n",
    "    })\n",
    "\n",
    "    # process data\n",
    "    gold = chat_template['messages'][-1]['content']\n",
    "    if not (gold_match := output_pattern.match(gold)):\n",
    "        raise ValueError(f\"Invalid gold output format: {gold}\")\n",
    "    if not (pred_match := output_pattern.match(pred)):\n",
    "        print(f\"Invalid prediction format:\\n\\n{pred}\\n\\n---------------------------\")\n",
    "        continue\n",
    "\n",
    "    pred_lines = pred_match.group(\"answer\").split(\"\\n\")\n",
    "    gold_lines = gold_match.group(\"answer\").split(\"\\n\")\n",
    "\n",
    "    # operator classification\n",
    "    pred_op_id = op_to_id.get(pred_lines[0].lower(), op_to_id[\"other\"])\n",
    "    gold_op_id = op_to_id.get(gold_lines[0].lower(), op_to_id[\"other\"])\n",
    "    y_pred.append(pred_op_id)\n",
    "    y_true.append(gold_op_id)\n",
    "\n",
    "    # sentence similarity\n",
    "    if pred_op_id == gold_op_id:\n",
    "        op_correct += 1\n",
    "\n",
    "        model_sentences = pred_lines[1:]\n",
    "        label_sentences = gold_lines[1:]\n",
    "\n",
    "        sentences_count += len(label_sentences)\n",
    "\n",
    "        if len(model_sentences) != len(label_sentences):\n",
    "            print(\"======================\")\n",
    "            print(pred)\n",
    "            print(\"----\")\n",
    "            print(gold)\n",
    "            print(\"======================\")\n",
    "        else:\n",
    "            all_sentences = model_sentences + label_sentences\n",
    "            embeddings = emb_model.encode(all_sentences, show_progress_bar=False)\n",
    "            similarity_matrix = embeddings @ embeddings.T\n",
    "            if len(model_sentences) == 0:\n",
    "                similarity = 0\n",
    "            elif len(model_sentences) == 1:\n",
    "                similarity = similarity_matrix[0][1]\n",
    "            else:\n",
    "                similarity = max(\n",
    "                    similarity_matrix[0,2] + similarity_matrix[1,3],\n",
    "                    similarity_matrix[0,3] + similarity_matrix[1,2]\n",
    "                ) / 2\n",
    "            sum_similarity += similarity\n",
    "\n",
    "print(f\"Operator Accuracy: {op_correct / len(test_dataset)}\")\n",
    "print(f\"Sentence Similarity (when operator is correct): {sum_similarity / op_correct}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Print nicely\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.index = df.index.map(lambda x: id_to_op[int(x)] if x.isdigit() else x)\n",
    "df['support'] = df['support'].astype(int)\n",
    "print(df)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(op_to_id.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(op_to_id.keys()))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add76510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('output/qpl/decomposer_3_predictions.json', 'w') as f:\n",
    "    json.dump(output_json, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
