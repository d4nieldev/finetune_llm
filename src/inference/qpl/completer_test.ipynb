{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c4c8b7",
   "metadata": {},
   "source": [
    "# Completer Test\n",
    "\n",
    "This notebook is made for assessing the performence of the QPL completer alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3a957",
   "metadata": {},
   "source": [
    "## Generate Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4217b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from src.prompters import PrompterRegistry\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.generation import to_model_prompt, generate_batch\n",
    "import src.utils.paths as p\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 6\n",
    "MAX_NEW_TOKENS = 128\n",
    "MODEL_CKPT = \"855d8cb9_gemma-3-4b-it-qpl-composer-ds_train_batch_size=1_gradient_accumulation_steps=8_learning_rate=0.0002_num_train_epochs=4_gradient_checkpointing=True_logging_steps=0.00125_save_steps=0.0625_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-3996\"\n",
    "MODEL_PATH = p.TRAINED_MODELS_DIR / MODEL_CKPT\n",
    "DATASET_ID = \"d4nieldev/qpl-completer-ds\"\n",
    "\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(MODEL_PATH, attn_implementation='eager').cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = list(load_dataset(DATASET_ID, split=\"validation\"))\n",
    "prompter = PrompterRegistry.get(DATASET_ID)(with_assistant=False)\n",
    "chat_templates = list(map(prompter.to_chat_template, test_dataset))\n",
    "prompts = list(map(lambda ct: to_model_prompt(tokenizer, ct), chat_templates))\n",
    "\n",
    "# Decompose questions\n",
    "outputs = generate_batch(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_prompts=prompts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    progress_bar=tqdm(total=len(prompts), desc=\"Completing QPL\"),\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddf002",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from src.inference.qpl.qpl_to_cte import flat_qpl_to_cte\n",
    "from src.inference.qpl.validate_qpl import execute_sql, same_rs\n",
    "\n",
    "connection_string = (\n",
    "    'Driver={ODBC Driver 18 for SQL Server};'\n",
    "    'Server=tcp:spider-sql.database.windows.net,1433;'\n",
    "    'Database=test;'\n",
    "    'Uid=iloveqpl;'\n",
    "    'Pwd=P4$$w0rd!;'\n",
    "    'Encrypt=yes;'\n",
    "    'TrustServerCertificate=no;'\n",
    "    'Connection Timeout=30;'\n",
    ")\n",
    "\n",
    "conn = pyodbc.connect(connection_string, autocommit=True)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "execution_accuracy = 0\n",
    "for out, example in tqdm(zip(outputs, test_dataset), desc=\"Evaluating\", total=len(outputs)):\n",
    "    gold = example['prefix_qpl'] + \"\\n\" + example['qpl_line']\n",
    "    pred = example['prefix_qpl'] + \"\\n\" + example['op'] + ' ' + out\n",
    "\n",
    "    flat_gold = [line[:line.index('--')] if '--' in line else line for line in gold.split('\\n')]\n",
    "    flat_pred = [line[:line.index('--')] if '--' in line else line for line in pred.split('\\n')]\n",
    "\n",
    "    gold_cte = flat_qpl_to_cte(flat_gold, example['db_id'])\n",
    "    pred_cte = flat_qpl_to_cte(flat_pred, example['db_id'])\n",
    "\n",
    "    grs = execute_sql(cursor, gold_cte)\n",
    "    prs = execute_sql(cursor, pred_cte)\n",
    "\n",
    "    same = same_rs(grs, prs, flat_pred)\n",
    "    if same:\n",
    "        execution_accuracy += 1\n",
    "\n",
    "print(f\"Execution accuracy: {execution_accuracy}/{len(outputs)} ({execution_accuracy / len(outputs) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "prompter = PrompterRegistry.get(DATASET_ID)(with_assistant=True)\n",
    "chat_templates = []\n",
    "for example in test_dataset:\n",
    "    chat_templates.append(prompter.to_chat_template(example))\n",
    "\n",
    "\n",
    "def equivalent_bracketed_lines(\n",
    "        true_line: str,\n",
    "        generated_line: str,\n",
    "        *,\n",
    "        require_exact_names: bool = False,\n",
    "        ignore_case: bool = True\n",
    "    ) -> bool:\n",
    "    _ZERO_DECIMAL = re.compile(r'(?<![\\d.])(\\d+)\\.0+\\b')\n",
    "    _TABLE_COL    = re.compile(r'#\\d+\\.\\s*[\\w$]+', flags=re.I)\n",
    "    _EQUALITY     = re.compile(rf'({_TABLE_COL.pattern})\\s*=\\s*({_TABLE_COL.pattern})', flags=re.I)\n",
    "\n",
    "    def _norm_nums(txt: str) -> str:\n",
    "        return _ZERO_DECIMAL.sub(r'\\1', txt)\n",
    "\n",
    "    # ---- build equivalence classes from join predicates\n",
    "    def _build_equiv_map(*lines: str) -> dict[str, str]:\n",
    "        parent = {}\n",
    "\n",
    "        def find(x):\n",
    "            parent.setdefault(x, x)\n",
    "            if parent[x] != x:\n",
    "                parent[x] = find(parent[x])\n",
    "            return parent[x]\n",
    "\n",
    "        def union(a, b):\n",
    "            ra, rb = find(a), find(b)\n",
    "            if ra != rb:\n",
    "                parent[rb] = ra\n",
    "\n",
    "        for line in lines:\n",
    "            for block in re.findall(r'Predicate\\s*\\[([^\\]]*)\\]',\n",
    "                                    _norm_nums(line), flags=re.I):\n",
    "                for left, right in _EQUALITY.findall(block):\n",
    "                    l = left.replace(' ', '')\n",
    "                    r = right.replace(' ', '')\n",
    "                    if ignore_case:\n",
    "                        l, r = l.upper(), r.upper()\n",
    "                    union(l, r)\n",
    "\n",
    "        equiv = {}\n",
    "        for full in parent:\n",
    "            col = full.split('.', 1)[1]\n",
    "            root = find(full)\n",
    "            equiv[full] = col.upper() if ignore_case else col\n",
    "            if root not in equiv:\n",
    "                equiv[root] = equiv[full]\n",
    "        return equiv\n",
    "\n",
    "    EQUIV = _build_equiv_map(true_line, generated_line)\n",
    "\n",
    "    # ---- compare skeletons (outside brackets)\n",
    "    def _skeleton(txt: str) -> str:\n",
    "        txt = _norm_nums(txt)\n",
    "        txt = re.sub(r'\\[[^\\]]*]', '[]', txt)\n",
    "        txt = re.sub(r'\\s+', ' ', txt).strip()\n",
    "        return txt.upper() if ignore_case else txt\n",
    "\n",
    "    if _skeleton(true_line) != _skeleton(generated_line):\n",
    "        return False\n",
    "\n",
    "    # ---- helper to extract bracket contents\n",
    "    blocks = lambda s: re.findall(r'\\[([^\\]]*)]', _norm_nums(s))\n",
    "\n",
    "    t_blocks, g_blocks = blocks(true_line), blocks(generated_line)\n",
    "    if len(t_blocks) != len(g_blocks):\n",
    "        return False\n",
    "\n",
    "    # ---- canonicalise individual tokens\n",
    "    def canon(tok: str) -> str:\n",
    "        tok = re.split(r'\\s+AS\\s+', tok, flags=re.I)[0]\n",
    "        tok = re.sub(r'\\s+', ' ', tok).strip()\n",
    "        tok = _norm_nums(tok)\n",
    "\n",
    "        def repl(m):\n",
    "            key = m.group(0).replace(' ', '')\n",
    "            key = key.upper() if ignore_case else key\n",
    "            return EQUIV.get(key, m.group(0))\n",
    "\n",
    "        tok = _TABLE_COL.sub(repl, tok)\n",
    "        return tok.upper() if ignore_case else tok\n",
    "\n",
    "    # ---- compare each corresponding block\n",
    "    for tb, gb in zip(t_blocks, g_blocks):\n",
    "        t_set = {canon(tok) for tok in tb.split(',') if tok.strip()}\n",
    "        g_set = {canon(tok) for tok in gb.split(',') if tok.strip()}\n",
    "\n",
    "        if require_exact_names:\n",
    "            if t_set != g_set:\n",
    "                return False\n",
    "        else:\n",
    "            if not t_set.issubset(g_set):\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "acc = 0\n",
    "for out, chat_template, example in tqdm(zip(outputs, chat_templates, test_dataset), desc=\"Evaluating\", total=len(outputs)):\n",
    "    label = chat_template['messages'][-1]['content']\n",
    "    equivalent = equivalent_bracketed_lines(\n",
    "        label,\n",
    "        out,\n",
    "    )\n",
    "    if not equivalent:\n",
    "        print(\"Question:\")\n",
    "        print(example['question'])\n",
    "        print('-'*40)\n",
    "        print(\"Predicted:\")\n",
    "        print(example['op'], out)\n",
    "        print('-'*40)\n",
    "        print(\"True:\")\n",
    "        print(example['op'], label)\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        acc += 1\n",
    "\n",
    "print(f\"Accuracy: {acc}/{len(outputs)} = {acc / len(outputs) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
