{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e3685",
   "metadata": {},
   "source": [
    "# 1. Load Model, Tokenizer, and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9dc8b",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from processors import EmotionProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"google/gemma-3-4b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85d44e",
   "metadata": {},
   "source": [
    "## TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856f253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41042eed7ef48acbc62b60182627165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8efc1a549b4399a01f54529c56ac9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parent not found for question: What is the first name and age of every student who has a dog but does not have a cat?\n",
      "WARNING:root:Parent not found for question: Find the name and production time of the cars that were produced in the earliest year.\n",
      "WARNING:root:Parent not found for question: What is the name of the cars produced in the earliest year and what year was it?\n",
      "WARNING:root:Parent not found for question: Which distinct car models have been produced after 1980?\n",
      "WARNING:root:Parent not found for question: What are the different models for the cars produced after 1980?\n",
      "WARNING:root:Parent not found for question: How many car makers are there in each continent? List the continent name and the count.\n",
      "WARNING:root:Parent not found for question: What is the acceleration of the car \"amc hornet sportabout (sw)\"?\n",
      "WARNING:root:Parent not found for question: How much does the amc hornet sportabout (sw) accelerate?\n",
      "WARNING:root:Parent not found for question: What is the number of makers of cars in France?\n",
      "WARNING:root:Parent not found for question: What is the average miles per gallon of all the cars with 4 cylinders?\n",
      "WARNING:root:Parent not found for question: What is the average weight of cars and year for each year?\n",
      "WARNING:root:Parent not found for question: What is the maximum horsepower and the name of the car models with 3 cylinders?\n",
      "WARNING:root:Parent not found for question: What is the largest amount of horsepower for the models with 3 cylinders and what name is it?\n",
      "WARNING:root:Parent not found for question: What is the average engine displacement of the volvo models?\n",
      "WARNING:root:Parent not found for question: What is the maximum acceleration for the each number of cylinders?\n",
      "WARNING:root:Parent not found for question: What is the maximum acceleration for all the different cylinders?\n",
      "WARNING:root:Parent not found for question: What is the horsepower of the car with the largest acceleration?\n",
      "WARNING:root:Parent not found for question: What is the horsepower of the car with the greatest acceleration?\n",
      "WARNING:root:Parent not found for question: For model volvo, how many cylinders does the car with the least acceleration have?\n",
      "WARNING:root:Parent not found for question: For a volvo model, how many cylinders does the version with least acceleration have?\n",
      "WARNING:root:Parent not found for question: How many cars have a larger acceleration than the car with the largest horsepower?\n",
      "WARNING:root:Parent not found for question: What is the number of cars with a greater acceleration than the one with the most horsepower?\n",
      "WARNING:root:Parent not found for question: How many countries have more than 2 car makers ?\n",
      "WARNING:root:Parent not found for question: What is the number of cars with over 6 cylinders?\n",
      "WARNING:root:Parent not found for question: Among the cars with more than lowest horsepower, which ones do not have more than 3 cylinders? List the car id and name.\n",
      "WARNING:root:Parent not found for question: Among the cars that do not have the minimum horsepower , what are the ids and names of all those with less than 4 cylinders?\n",
      "WARNING:root:Parent not found for question: What are the different models that are lighter than 3500 but were not built by the Ford Motor Company?\n",
      "WARNING:root:Parent not found for question: Which are the car makers which produce at least 2 models and more than 3 cars? List the id and the maker.\n",
      "WARNING:root:Parent not found for question: What are the ids and names of all car makers that produce at least 2 models and make more than 3 cars?\n",
      "WARNING:root:Parent not found for question: What are the ids and names of the countries which have more than 3 car makers or produce the 'fiat' model?\n",
      "WARNING:root:Parent not found for question: Which city is the most frequent flight origin?\n",
      "WARNING:root:Parent not found for question: What is the semester in which most students registered? Show both the name and the id.\n",
      "WARNING:root:Parent not found for question: What is the number of different addresses that have students living there?\n",
      "WARNING:root:Parent not found for question: find id of the tv channels that from the countries where have more than two tv channels.\n",
      "WARNING:root:Parent not found for question: What are the ids of all tv channels in countries that have more than 2 TV channels?\n",
      "WARNING:root:Parent not found for question: What are the names of the countries that are in the continent of Europe and have a population of 80000?\n",
      "WARNING:root:Parent not found for question: Give the names of countries that are in Europe and have a population equal to 80000.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from processors import QPLDecomposerProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/gemma-3-4b-it-question_decomposer_ds_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-20958\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"bgunlp/question_decomposer_ds\", split=\"validation\")\n",
    "processor = QPLDecomposerProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a4008",
   "metadata": {},
   "source": [
    "## Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083adc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading unsloth\n",
    "from unsloth import FastLanguageModel, get_chat_template\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset\n",
    "from processors import EmotionProcessor\n",
    "\n",
    "add_special_tokens = True\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/unsloth/gemma-3-4b-it-emotion_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-32000\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=50,\n",
    "    load_in_4bit=False,\n",
    "    dtype=None,\n",
    ")\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\"\n",
    ")\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return {'prompt': prompt, 'labels': example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a53e75",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03c41a",
   "metadata": {},
   "source": [
    "## One Example Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d94a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one example\n",
    "model_inputs = tokenizer(\n",
    "    [test_dataset[405][\"prompt\"]], \n",
    "    padding=True,\n",
    "    padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=add_special_tokens,\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]  # remove the input ids\n",
    "model_first_output = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)[0]\n",
    "print(model_first_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6db08c",
   "metadata": {},
   "source": [
    "## Test Dataset - Strict Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=add_special_tokens,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        correct += sum([1 if model_outputs[j] == batch[\"labels\"][j] else 0 for j in range(len(batch['prompt']))])\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ef6d2",
   "metadata": {},
   "source": [
    "## Test Dataset - Flexible Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "labels = [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        for j in range(len(batch['prompt'])):\n",
    "            label = batch[\"labels\"][j][34:]\n",
    "            model_output = model_outputs[j].lower()\n",
    "            model_answers = [model_output.find(label) for label in labels]\n",
    "            try:\n",
    "                best_answer = model_answers.index(min([ans for ans in model_answers if ans != -1]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            model_prediction = labels[best_answer]\n",
    "            if model_prediction == label:\n",
    "                correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
