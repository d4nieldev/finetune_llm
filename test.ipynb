{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e3685",
   "metadata": {},
   "source": [
    "# 1. Load Model, Tokenizer, and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9dc8b",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from processors import EmotionProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"google/gemma-3-4b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85d44e",
   "metadata": {},
   "source": [
    "## TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from processors import QPLDecomposerProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/gemma-3-4b-it-question_decomposer_ds_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-20958\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"bgunlp/question_decomposer_ds\", split=\"validation\")\n",
    "processor = QPLDecomposerProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a4008",
   "metadata": {},
   "source": [
    "## Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083adc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading unsloth\n",
    "from unsloth import FastModel, get_chat_template, standardize_data_formats\n",
    "from datasets import load_dataset\n",
    "from processors import EmotionProcessor\n",
    "\n",
    "add_special_tokens = True\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/unsloth/gemma-3-4b-it-emotion_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05_2025-05-01_12-56-00/checkpoint-32000\"\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=False,\n",
    "    dtype=None,\n",
    ")\n",
    "model = FastModel.for_inference(model)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\"\n",
    ")\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "test_dataset = test_dataset.rename_column(\"messages\", \"conversations\")\n",
    "test_dataset = standardize_data_formats(test_dataset)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"conversations\"][0]],  # only the user\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return {'prompt': prompt, 'labels': example[\"conversations\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a53e75",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03c41a",
   "metadata": {},
   "source": [
    "## One Example Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d94a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one example\n",
    "example = test_dataset[1362]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    [example[\"prompt\"]], \n",
    "    padding=True,\n",
    "    padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=add_special_tokens,\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=False)\n",
    "generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]  # remove the input ids\n",
    "model_first_output = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"## Prompt\")\n",
    "print(example[\"prompt\"])\n",
    "print(\"\\n\\n## Labels\")\n",
    "print(example[\"labels\"])\n",
    "print(\"\\n\\n## Model Output\")\n",
    "print(model_first_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae053da",
   "metadata": {},
   "source": [
    "## Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6db08c",
   "metadata": {},
   "source": [
    "### Strict Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=add_special_tokens,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        correct += sum([1 if model_outputs[j] == batch[\"labels\"][j] else 0 for j in range(len(batch['prompt']))])\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ef6d2",
   "metadata": {},
   "source": [
    "### Flexible Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "labels = [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        for j in range(len(batch['prompt'])):\n",
    "            label = batch[\"labels\"][j][34:]\n",
    "            model_output = model_outputs[j].lower()\n",
    "            model_answers = [model_output.find(label) for label in labels]\n",
    "            try:\n",
    "                best_answer = model_answers.index(min([ans for ans in model_answers if ans != -1]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            model_prediction = labels[best_answer]\n",
    "            if model_prediction == label:\n",
    "                correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6dafd",
   "metadata": {},
   "source": [
    "## Test Decomposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bsz = 8\n",
    "op_correct = 0\n",
    "sum_similarity = 0\n",
    "sentences_count = 0\n",
    "\n",
    "op_to_id = {\n",
    "    'aggregate': 0,\n",
    "    'except': 1,\n",
    "    'filter': 2,\n",
    "    'intersect': 3,\n",
    "    'join': 4,\n",
    "    'scan': 5,\n",
    "    'sort': 6,\n",
    "    'topsort': 7,\n",
    "    'union': 8,\n",
    "    'other': 9\n",
    "}\n",
    "id_to_op = {v: k for k, v in op_to_id.items()}\n",
    "\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=add_special_tokens,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "\n",
    "        for model_output, label in zip(model_outputs, batch[\"labels\"]):\n",
    "            model_lines = model_output.split(\"\\n\")\n",
    "            label_lines = label.split(\"\\n\")\n",
    "\n",
    "            model_op_id = op_to_id.get(model_lines[0].lower(), op_to_id[\"other\"])\n",
    "            label_op_id = op_to_id.get(label_lines[0].lower(), op_to_id[\"other\"])\n",
    "            y_pred.append(model_op_id)\n",
    "            y_true.append(label_op_id)\n",
    "\n",
    "            if model_op_id == label_op_id:\n",
    "                op_correct += 1\n",
    "\n",
    "                model_sentences = model_lines[1:]\n",
    "                label_sentences = label_lines[1:]\n",
    "\n",
    "                sentences_count += len(label_sentences)\n",
    "\n",
    "                if len(model_sentences) != len(label_sentences):\n",
    "                    print(\"======================\")\n",
    "                    print(model_output)\n",
    "                    print(\"----\")\n",
    "                    print(label)\n",
    "                    print(\"======================\")\n",
    "                else:\n",
    "                    all_sentences = model_sentences + label_sentences\n",
    "                    embeddings = emb_model.encode(all_sentences)\n",
    "                    similarity_matrix = embeddings @ embeddings.T\n",
    "                    if len(model_sentences) == 0:\n",
    "                        similarity = 0\n",
    "                    elif len(model_sentences) == 1:\n",
    "                        similarity = similarity_matrix[0][1]\n",
    "                    else:\n",
    "                        similarity = max(\n",
    "                            similarity_matrix[0,2] + similarity_matrix[1,3],\n",
    "                            similarity_matrix[0,3] + similarity_matrix[1,2]\n",
    "                        ) / 2\n",
    "                    sum_similarity += similarity\n",
    "\n",
    "print(f\"Operator Accuracy: {op_correct / len(test_dataset)}\")\n",
    "print(f\"Sentence Similarity (when operator is correct): {sum_similarity / op_correct}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Print nicely\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.index = df.index.map(lambda x: id_to_op[int(x)] if x.isdigit() else x)\n",
    "df['support'] = df['support'].astype(int)\n",
    "print(df)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(op_to_id.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(op_to_id.keys()))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
