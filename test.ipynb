{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e3685",
   "metadata": {},
   "source": [
    "# 1. Load Model, Tokenizer, and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9dc8b",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from processors import EmotionProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"google/gemma-3-4b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85d44e",
   "metadata": {},
   "source": [
    "## TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8856f253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86011ebfc4d44b21bc5e7b97c6a6d4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7112a272853a43208b4b5acd81ebbe3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from processors import ProcessorRegistry\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "model_path = \"output/erbz0056_gemma-3-4b-it-qpl_composer_train_batch_size=1_gradient_accumulation_steps=8_learning_rate=0.0002_num_train_epochs=4_gradient_checkpointing=True_logging_steps=0.05_save_steps=0.5_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-5316\"\n",
    "dataset_id = \"d4nieldev/qpl_composer\"\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(dataset_id, split=\"validation\")\n",
    "processor = ProcessorRegistry.get(dataset_id)()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [msg for msg in example[\"messages\"] if msg['role'] in ['system', 'user']],  # only the system and user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": next(msg['content'] for msg in example['messages'] if msg['role'] == 'assistant')}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a4008",
   "metadata": {},
   "source": [
    "## Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083adc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading unsloth\n",
    "from unsloth import FastModel, get_chat_template, standardize_data_formats\n",
    "from datasets import load_dataset\n",
    "from processors import EmotionProcessor\n",
    "\n",
    "add_special_tokens = True\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/unsloth/gemma-3-4b-it-emotion_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05_2025-05-01_12-56-00/checkpoint-32000\"\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=False,\n",
    "    dtype=None,\n",
    ")\n",
    "model = FastModel.for_inference(model)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\"\n",
    ")\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "test_dataset = test_dataset.rename_column(\"messages\", \"conversations\")\n",
    "test_dataset = standardize_data_formats(test_dataset)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"conversations\"][0]],  # only the user\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return {'prompt': prompt, 'labels': example[\"conversations\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a53e75",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03c41a",
   "metadata": {},
   "source": [
    "## One Example Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d94a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Prompt\n",
      "<bos><start_of_turn>user\n",
      "Given a database schema, a QPL query prefix, and a natural language question, complete the final line of the query so it completes the user request.\n",
      "\n",
      "QPL is a formalism used to describe data retrieval operations over an SQL schema in a modular manner.\n",
      "A QPL plan is a sequence of instructions for querying tabular data to answer a natural language question.\n",
      "\n",
      "Below is the formal specification for each operation in valid QPL:\n",
      "<qpl> ::= <line>+\n",
      "<line> ::= #<integer> = <operator>\n",
      "<operator> ::= <scan> | <aggregate> | <filter> | <sort> | <topsort> | <join> | <except> | <intersect> | <union>\n",
      "\n",
      "-- Leaf operator\n",
      "<scan> ::= Scan Table [ <table-name> ] <pred>? <distinct>? <output-non-qualif>\n",
      "\n",
      "-- Unary operators\n",
      "<aggregate> ::= Aggregate [ <input> ] <group-by>? <output-non-qualif>\n",
      "<filter> ::= Filter [ <input> ] <pred> <distinct>? <output-non-qualif>\n",
      "<sort> ::= Sort [ <input> ] <order-by> <withTie>? <output-non-qualif>\n",
      "<topsort> ::= TopSort [ <input> ] Rows [ <number> ] <order-by> <withTies>? <output-non-qualif>\n",
      "\n",
      "-- Binary operators\n",
      "<join> ::= Join [ <input> , <input> ] <pred>? <distinct>? <output-qualif>\n",
      "<except> ::= Except [ <input> , <input> ] <pred> <output-qualif>\n",
      "<intersect> ::= Intersect [ <input> , <input> ] <pred>? <output-qualif>\n",
      "<union> ::= Union [ <input> , <input> ] <output-qualif>\n",
      "\n",
      "<group-by> ::= GroupBy [ <column-name> (, <column-name>)* ]\n",
      "<order-by> ::= OrderBy [ <column-name> <direction> (, <column-name> <direction>)* ]\n",
      "<withTies> ::= WithTies [ true | false ]\n",
      "<direction> ::= ASC | DESC\n",
      "<pred> ::= Predicate [ <comparison> (AND | OR <comparison)* ]\n",
      "<distinct> ::= Distinct [ true | false ]\n",
      "<output-non-qualif> ::= Output [ <column-name> (, <column-name>)* ]\n",
      "<output-qualif> ::= Output [ <qualif-column-name> (, <qualif-column-name>)* ]\n",
      "<qualif-column-name> ::= # <number> . <column-name>\n",
      "\n",
      "Database Name: concert_singer\n",
      "\n",
      "Database Schema:\n",
      "```DDL\n",
      "CREATE TABLE stadium (\n",
      "\tStadium_ID number,\n",
      "\tLocation text,\n",
      "\tName text,\n",
      "\tCapacity number,\n",
      "\tHighest number,\n",
      "\tLowest number,\n",
      "\tAverage number,\n",
      "\tprimary key ( Stadium_ID )\n",
      ")\n",
      "\n",
      "CREATE TABLE singer (\n",
      "\tSinger_ID number,\n",
      "\tName text,\n",
      "\tCountry text,\n",
      "\tSong_Name text,\n",
      "\tSong_release_year text,\n",
      "\tAge number,\n",
      "\tIs_male text,\n",
      "\tprimary key ( Singer_ID )\n",
      ")\n",
      "\n",
      "CREATE TABLE concert (\n",
      "\tconcert_ID number,\n",
      "\tconcert_Name text,\n",
      "\tTheme text,\n",
      "\tStadium_ID number,\n",
      "\tYear number,\n",
      "\tprimary key ( concert_ID ),\n",
      "\tforeign key ( Stadium_ID ) references stadium ( Stadium_ID )\n",
      ")\n",
      "\n",
      "CREATE TABLE singer_in_concert (\n",
      "\tconcert_ID number,\n",
      "\tSinger_ID number,\n",
      "\tprimary key ( concert_ID , Singer_ID ),\n",
      "\tforeign key ( Singer_ID ) references singer ( Singer_ID ),\n",
      "\tforeign key ( concert_ID ) references concert ( concert_ID )\n",
      ")```\n",
      "\n",
      "Question: Show the id, name and capacity of all stadiums.\n",
      "\n",
      "The QPL query that satisfies the question's intent is:\n",
      "\n",
      "```QPL\n",
      "#1 = Scan<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "\n",
      "## Labels\n",
      "#2 = Scan Table [ stadium ] Output [ Capacity , Stadium_ID , Name ]\n",
      "```\n",
      "\n",
      "\n",
      "## Model Output\n",
      "#2 = Scan Table [ stadium ] Output [ Stadium_ID , Capacity , Name ]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# one example\n",
    "example = test_dataset[53]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    [example[\"prompt\"]], \n",
    "    padding=True,\n",
    "    padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=add_special_tokens,\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=False)\n",
    "generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]  # remove the input ids\n",
    "model_first_output = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"## Prompt\")\n",
    "print(example[\"prompt\"])\n",
    "print(\"\\n\\n## Labels\")\n",
    "print(example[\"labels\"])\n",
    "print(\"\\n\\n## Model Output\")\n",
    "print(model_first_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae053da",
   "metadata": {},
   "source": [
    "## Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6db08c",
   "metadata": {},
   "source": [
    "### Strict Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=add_special_tokens,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        correct += sum([1 if model_outputs[j] == batch[\"labels\"][j] else 0 for j in range(len(batch['prompt']))])\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ef6d2",
   "metadata": {},
   "source": [
    "### Flexible Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "labels = [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        for j in range(len(batch['prompt'])):\n",
    "            label = batch[\"labels\"][j][34:]\n",
    "            model_output = model_outputs[j].lower()\n",
    "            model_answers = [model_output.find(label) for label in labels]\n",
    "            try:\n",
    "                best_answer = model_answers.index(min([ans for ans in model_answers if ans != -1]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            model_prediction = labels[best_answer]\n",
    "            if model_prediction == label:\n",
    "                correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6dafd",
   "metadata": {},
   "source": [
    "## Test Decomposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bsz = 8\n",
    "op_correct = 0\n",
    "sum_similarity = 0\n",
    "sentences_count = 0\n",
    "\n",
    "op_to_id = {\n",
    "    'aggregate': 0,\n",
    "    'except': 1,\n",
    "    'filter': 2,\n",
    "    'intersect': 3,\n",
    "    'join': 4,\n",
    "    'scan': 5,\n",
    "    'sort': 6,\n",
    "    'topsort': 7,\n",
    "    'union': 8,\n",
    "    'other': 9\n",
    "}\n",
    "id_to_op = {v: k for k, v in op_to_id.items()}\n",
    "\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=add_special_tokens,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "\n",
    "        for model_output, label in zip(model_outputs, batch[\"labels\"]):\n",
    "            model_lines = model_output.split(\"\\n\")\n",
    "            label_lines = label.split(\"\\n\")\n",
    "\n",
    "            model_op_id = op_to_id.get(model_lines[0].lower(), op_to_id[\"other\"])\n",
    "            label_op_id = op_to_id.get(label_lines[0].lower(), op_to_id[\"other\"])\n",
    "            y_pred.append(model_op_id)\n",
    "            y_true.append(label_op_id)\n",
    "\n",
    "            if model_op_id == label_op_id:\n",
    "                op_correct += 1\n",
    "\n",
    "                model_sentences = model_lines[1:]\n",
    "                label_sentences = label_lines[1:]\n",
    "\n",
    "                sentences_count += len(label_sentences)\n",
    "\n",
    "                if len(model_sentences) != len(label_sentences):\n",
    "                    print(\"======================\")\n",
    "                    print(model_output)\n",
    "                    print(\"----\")\n",
    "                    print(label)\n",
    "                    print(\"======================\")\n",
    "                else:\n",
    "                    all_sentences = model_sentences + label_sentences\n",
    "                    embeddings = emb_model.encode(all_sentences)\n",
    "                    similarity_matrix = embeddings @ embeddings.T\n",
    "                    if len(model_sentences) == 0:\n",
    "                        similarity = 0\n",
    "                    elif len(model_sentences) == 1:\n",
    "                        similarity = similarity_matrix[0][1]\n",
    "                    else:\n",
    "                        similarity = max(\n",
    "                            similarity_matrix[0,2] + similarity_matrix[1,3],\n",
    "                            similarity_matrix[0,3] + similarity_matrix[1,2]\n",
    "                        ) / 2\n",
    "                    sum_similarity += similarity\n",
    "\n",
    "print(f\"Operator Accuracy: {op_correct / len(test_dataset)}\")\n",
    "print(f\"Sentence Similarity (when operator is correct): {sum_similarity / op_correct}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Print nicely\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.index = df.index.map(lambda x: id_to_op[int(x)] if x.isdigit() else x)\n",
    "df['support'] = df['support'].astype(int)\n",
    "print(df)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(op_to_id.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(op_to_id.keys()))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
