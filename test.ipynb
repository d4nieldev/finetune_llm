{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e3685",
   "metadata": {},
   "source": [
    "# 1. Load Model, Tokenizer, and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9dc8b",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from processors import EmotionProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"google/gemma-3-4b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85d44e",
   "metadata": {},
   "source": [
    "## TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8856f253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41042eed7ef48acbc62b60182627165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8efc1a549b4399a01f54529c56ac9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parent not found for question: What is the first name and age of every student who has a dog but does not have a cat?\n",
      "WARNING:root:Parent not found for question: Find the name and production time of the cars that were produced in the earliest year.\n",
      "WARNING:root:Parent not found for question: What is the name of the cars produced in the earliest year and what year was it?\n",
      "WARNING:root:Parent not found for question: Which distinct car models have been produced after 1980?\n",
      "WARNING:root:Parent not found for question: What are the different models for the cars produced after 1980?\n",
      "WARNING:root:Parent not found for question: How many car makers are there in each continent? List the continent name and the count.\n",
      "WARNING:root:Parent not found for question: What is the acceleration of the car \"amc hornet sportabout (sw)\"?\n",
      "WARNING:root:Parent not found for question: How much does the amc hornet sportabout (sw) accelerate?\n",
      "WARNING:root:Parent not found for question: What is the number of makers of cars in France?\n",
      "WARNING:root:Parent not found for question: What is the average miles per gallon of all the cars with 4 cylinders?\n",
      "WARNING:root:Parent not found for question: What is the average weight of cars and year for each year?\n",
      "WARNING:root:Parent not found for question: What is the maximum horsepower and the name of the car models with 3 cylinders?\n",
      "WARNING:root:Parent not found for question: What is the largest amount of horsepower for the models with 3 cylinders and what name is it?\n",
      "WARNING:root:Parent not found for question: What is the average engine displacement of the volvo models?\n",
      "WARNING:root:Parent not found for question: What is the maximum acceleration for the each number of cylinders?\n",
      "WARNING:root:Parent not found for question: What is the maximum acceleration for all the different cylinders?\n",
      "WARNING:root:Parent not found for question: What is the horsepower of the car with the largest acceleration?\n",
      "WARNING:root:Parent not found for question: What is the horsepower of the car with the greatest acceleration?\n",
      "WARNING:root:Parent not found for question: For model volvo, how many cylinders does the car with the least acceleration have?\n",
      "WARNING:root:Parent not found for question: For a volvo model, how many cylinders does the version with least acceleration have?\n",
      "WARNING:root:Parent not found for question: How many cars have a larger acceleration than the car with the largest horsepower?\n",
      "WARNING:root:Parent not found for question: What is the number of cars with a greater acceleration than the one with the most horsepower?\n",
      "WARNING:root:Parent not found for question: How many countries have more than 2 car makers ?\n",
      "WARNING:root:Parent not found for question: What is the number of cars with over 6 cylinders?\n",
      "WARNING:root:Parent not found for question: Among the cars with more than lowest horsepower, which ones do not have more than 3 cylinders? List the car id and name.\n",
      "WARNING:root:Parent not found for question: Among the cars that do not have the minimum horsepower , what are the ids and names of all those with less than 4 cylinders?\n",
      "WARNING:root:Parent not found for question: What are the different models that are lighter than 3500 but were not built by the Ford Motor Company?\n",
      "WARNING:root:Parent not found for question: Which are the car makers which produce at least 2 models and more than 3 cars? List the id and the maker.\n",
      "WARNING:root:Parent not found for question: What are the ids and names of all car makers that produce at least 2 models and make more than 3 cars?\n",
      "WARNING:root:Parent not found for question: What are the ids and names of the countries which have more than 3 car makers or produce the 'fiat' model?\n",
      "WARNING:root:Parent not found for question: Which city is the most frequent flight origin?\n",
      "WARNING:root:Parent not found for question: What is the semester in which most students registered? Show both the name and the id.\n",
      "WARNING:root:Parent not found for question: What is the number of different addresses that have students living there?\n",
      "WARNING:root:Parent not found for question: find id of the tv channels that from the countries where have more than two tv channels.\n",
      "WARNING:root:Parent not found for question: What are the ids of all tv channels in countries that have more than 2 TV channels?\n",
      "WARNING:root:Parent not found for question: What are the names of the countries that are in the continent of Europe and have a population of 80000?\n",
      "WARNING:root:Parent not found for question: Give the names of countries that are in Europe and have a population equal to 80000.\n",
      "WARNING:root:Parent not found for question: Tell me the owner id and last name of the owner who did the most treatments of his or her dogs.\n",
      "WARNING:root:Parent not found for question: Which professionals have operated a treatment that costs less than the average? Give me their first names and last names.\n",
      "WARNING:root:Parent not found for question: List each owner's first name, last name, and the size of his or her dog.\n",
      "WARNING:root:Parent not found for question: What is the name of every singer that does not have any song?\n",
      "WARNING:root:Parent not found for question: Show the property type descriptions of all properties?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef16cc320e741ca961d9d7a3976e448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from processors import QPLDecomposerProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "add_special_tokens = False\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/gemma-3-4b-it-question_decomposer_ds_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05/checkpoint-20958\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"bgunlp/question_decomposer_ds\", split=\"validation\")\n",
    "processor = QPLDecomposerProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"][0]],  # only the user\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"labels\": example[\"messages\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a4008",
   "metadata": {},
   "source": [
    "## Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083adc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78f347df92649428e95e747dd501d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4233850700b4eb4973fa9c09c214f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading unsloth\n",
    "from unsloth import FastModel, get_chat_template, standardize_data_formats\n",
    "from datasets import load_dataset\n",
    "from processors import EmotionProcessor\n",
    "\n",
    "add_special_tokens = True\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_path = \"output/unsloth/gemma-3-4b-it-emotion_train_batch_size=1_gradient_accumulation_steps=1_learning_rate=0.0002_num_train_epochs=2_gradient_checkpointing=False_logging_steps=500_save_steps=5000_random_seed=1_lora=True_r=16_alpha=32_dropout=0.05_2025-05-01_12-56-00/checkpoint-32000\"\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=False,\n",
    "    dtype=None,\n",
    ")\n",
    "model = FastModel.for_inference(model)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\"\n",
    ")\n",
    "\n",
    "# Load and process data\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "processor = EmotionProcessor()\n",
    "test_dataset = test_dataset.map(processor.to_chat_template, remove_columns=test_dataset.column_names)\n",
    "test_dataset = test_dataset.rename_column(\"messages\", \"conversations\")\n",
    "test_dataset = standardize_data_formats(test_dataset)\n",
    "def to_model_prompt(example):\n",
    "    # example[\"messages\"] is a list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [example[\"conversations\"][0]],  # only the user\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return {'prompt': prompt, 'labels': example[\"conversations\"][1][\"content\"]}\n",
    "test_dataset = test_dataset.map(to_model_prompt, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a53e75",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03c41a",
   "metadata": {},
   "source": [
    "## One Example Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d94a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danieloh/.conda/envs/finetuning/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/danieloh/.conda/envs/finetuning/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Prompt\n",
      "<bos><start_of_turn>user\n",
      "Below is a piece of text. Classify it into one of: sadness, joy, love, anger, fear, surprise.\n",
      "\n",
      "\"im feeling rather rotten so im not very ambitious right now\"<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "\n",
      "## Labels\n",
      "The emotion in the above text is: sadness\n",
      "\n",
      "\n",
      "## Model Output\n",
      "The emotion in the following the opposite in the following thel only sadness despite the realming the realming the emotion in the following the newly good sadness in the Ex  sadnessing sadness in the Namibenseing happiness emotions in the Namib equing sadness in the following the emotion in the following theisticus againing te sadness in thelr emotional sadness in the Examin in the kingdoming happiness sadness/ sadness  sadness and sadness and sadnessys: sadness alone sadness and sadness! sadnessing sadness os: sadness  sadness  sadness  sadness  sadnessing sadness  sadnessing sadnessing sadnessing sadness and sadnessing sadness in the out er sadness> sadness  sadness  sadness in the Figure is: sadnessing sadness! sadness with joy emotions in the pil sadness with joy joy good\n",
      " emotion in the Emperor hunger art of sadness  sadness  sadness Despite sadness  sadness  sadness  sadnessob joy to sadness  sadness  sadness </ or sadness e good mood in the King how: sadnessing e good sadness\n"
     ]
    }
   ],
   "source": [
    "# one example\n",
    "example = test_dataset[0]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    [example[\"prompt\"]], \n",
    "    padding=True,\n",
    "    padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=add_special_tokens,\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=False)\n",
    "generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]  # remove the input ids\n",
    "model_first_output = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"## Prompt\")\n",
    "print(example[\"prompt\"])\n",
    "print(\"\\n\\n## Labels\")\n",
    "print(example[\"labels\"])\n",
    "print(\"\\n\\n## Model Output\")\n",
    "print(model_first_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6db08c",
   "metadata": {},
   "source": [
    "## Test Dataset - Strict Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=add_special_tokens,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        correct += sum([1 if model_outputs[j] == batch[\"labels\"][j] else 0 for j in range(len(batch['prompt']))])\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ef6d2",
   "metadata": {},
   "source": [
    "## Test Dataset - Flexible Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "labels = [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]\n",
    "\n",
    "bsz = 16\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), bsz)):\n",
    "        batch = test_dataset[i:i+ bsz]\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"prompt\"], \n",
    "            padding=True,\n",
    "            padding_side=\"left\",  # https://huggingface.co/docs/transformers/llm_tutorial?padding=right+pad#padding-side\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        ).to(\"cuda\")\n",
    "        generation_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "        generation_ids = generation_ids[:, model_inputs[\"input_ids\"].shape[1]:]\n",
    "        model_outputs = tokenizer.batch_decode(generation_ids, skip_special_tokens=True)\n",
    "        for j in range(len(batch['prompt'])):\n",
    "            label = batch[\"labels\"][j][34:]\n",
    "            model_output = model_outputs[j].lower()\n",
    "            model_answers = [model_output.find(label) for label in labels]\n",
    "            try:\n",
    "                best_answer = model_answers.index(min([ans for ans in model_answers if ans != -1]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            model_prediction = labels[best_answer]\n",
    "            if model_prediction == label:\n",
    "                correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
